<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="../css/index.css">
    <title>Experimenting with Fraud Detection - Arsh Singh</title>
</head>
<body>
    <div class="main">
        <div class="header">Arsh Singh</div>
        <div class="home">
            <h1>Experimenting with Fraud Detection</h1>
            <p>
                My goal was to get more familiar with PyTorch, and also to assess current offerings for detecting fraudulent images primarily generated by new modeling techniques.
            </p>
            <p>
                <strong>Hypothesis:</strong> I can train a model that is able to classify an image to be either AI generated or not. I can reduce the problem set to just images of cars and car accidents to make this a bit easier. I can rely on deep learning techniques to make this problem a bit easier.
            </p>

            <h2>Getting the Data</h2>
            <p>
                I needed data to start any of this. Luckily I had some friends who were able to generate images of accidents via GPT, DALL-E, Nova, and Titan. Unfortunately that combined with the real images I selected was around 578 rows.
            </p>
            <p>
                To use the data that was all housed in AWS S3, I created a dataframe that recursively fetched paths in the folders and created a final dataframe that had the file_path and label. This is important because instead of downloading everything locally I wanted to try and create a dataloader that fetched my files for me. I realized that some of the files I scraped myself had odd formats and had to do a cleaning step. Luckily since it was all in one dataframe I could do something like <code>df[~df['file_path'].str.endswith('x')].reset_index(drop=True)</code>. I then did a shuffle and split of my data. I had to add some additional logic to it considering my data was heavily skewed towards AI generated images.
            </p>

            <h2>Building the Dataset</h2>
            <p>Finally I implemented my dataset class:</p>
            <pre><code>class CarImageDataset(Dataset):
    def __init__(self, annotations_file, transform=None):
        self.img_labels = annotations_file
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.ConvertImageDtype(torch.float),
                transforms.Normalize(
                    mean=(0.485, 0.456, 0.406),
                    std=(0.229, 0.224, 0.225)
                )
            ])
        else:
            self.transform = transform

    def load_from_s3(self, s3_path):
        client = boto3.client('s3')
        bucket, key = s3_path[5:].split('/', 1)

        resp = client.get_object(Bucket=bucket, Key=key)
        data = resp['Body'].read()

        try:
            image = Image.open(BytesIO(data)).convert('RGB')
        except Exception as e:
            print(f'Error: Load failed for file {s3_path}.')

        return image

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = self.img_labels['file_path'].iloc[idx]
        image = self.load_from_s3(img_path)
        label = self.img_labels['label'].iloc[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

data = CarImageDataset(train)
dataloader = DataLoader(data, batch_size=64, num_workers=0, shuffle=True)</code></pre>

            <p>
                Btw this is clearly not optimal, things to revise would be to add some caching, change the default transform to be even higher quality, and a lot of other small things.
            </p>

            <h2>Choosing a Model</h2>
            <p>
                Next I had to choose a model. In this case I picked ResNet-18 with IMAGENET1K_V1 weights and changed the output to be 2:
            </p>
            <pre><code>def load_model(weights='IMAGENET1K_V1', n_out=2):
    model = models.resnet18(weights=weights)
    model.fc = nn.Linear(model.fc.in_features, n_out, bias=True)
    return model</code></pre>

            <p>
                I also determined a loss function. At first I started with <code>nn.CrossEntropyLoss()</code>, after trying it out I went to <code>nn.BCEWithLogitsLoss()</code>. I also opted for using AdamW since it's the mathematically correct version—there is a bug in Adam, I don't know why Adam isn't removed.
            </p>
            <pre><code>loss = nn.BCEWithLogitsLoss()

optimizer = optim.AdamW(model.parameters(), lr=0.001)</code></pre>

            <h2>Training Loop</h2>
            <p>My training loop was somewhat basic:</p>
            <pre><code>device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'
model.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    loop = tqdm(dataloader, leave=True)

    for images, labels in loop:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = model(images)

        y_onehot = nn.functional.one_hot(labels, num_classes=2).float()

        loss_val = loss(outputs, y_onehot)

        running_loss += loss_val.item()
        loss_val.backward()
        optimizer.step()

        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)

        correct += (predicted == labels).sum().item()

        ...</code></pre>

            <h2>Results and Conclusion</h2>
            <p>
                With this setup I had some mixed results around 60-80% depending on the shuffling of data.
            </p>
            <p>
                My conclusion is that I could predict if something was AI generated or not, if I scoped the problem down by a lot. I also should not use ResNet with such a small training set—it has roughly ~12 million parameters so fine tuning off a small amount was not great. Also I need better data, like it should be more balanced with fake and real images.
            </p>
        </div>
    </div>
</body>
</html>
